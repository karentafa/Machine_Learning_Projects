# **Data Quality Analysis (DQA)**




#IMPORT AND MOUNT

train_census = '/content/drive/MyDrive/MSBA_Assignments/CA03/train_census.csv'
test_census = '/content/drive/MyDrive/MSBA_Assignments/CA03/test_census.csv'
census_data = '/content/drive/MyDrive/MSBA_Assignments/CA03/census_data.csv'


import numpy as np
import pandas as pd
import seaborn as sns



#convert to dataframes

traindf = pd.read_csv('/content/drive/MyDrive/MSBA_Assignments/CA03/train_census.csv')
testdf = pd.read_csv('/content/drive/MyDrive/MSBA_Assignments/CA03/test_census.csv')
datadf = pd.read_csv('/content/drive/MyDrive/MSBA_Assignments/CA03/census_data.csv')

datadf.head()

**Missing Values**


#find missing values
datadf.isnull().sum()


datadf.head()

#tried to check outliers but since they require numerical values we cant

**Descriptive Stats**

#2

#describe hours_per_week_bin
datadf.hours_per_week_bin.describe()

#describe occupation_bin

datadf.occupation_bin.describe()


#describe msr_bin
datadf.msr_bin.describe()

#describe capital_gl_bin

datadf.capital_gl_bin.describe()

#describe race_sex_bin

datadf.race_sex_bin.describe()

#describe education_num_bin


datadf.education_num_bin.describe()

#describe education_bin

datadf.education_bin.describe()

#describe workclass_bin

datadf.workclass_bin.describe()

#describe age_bin

datadf.age_bin.describe()



**Data Cleaning**

#not needed

# **Explanatory Data Analysis (EDA)**

#Age

datadf.pivot_table(index= 'age_bin', columns= 'y', aggfunc= 'count', values=['flag'],  fill_value=0
                  ).apply(lambda x: x/ x.sum() * 100, axis=1).plot.bar(stacked= True)

#Capital Gain/Loss

datadf.pivot_table(index= 'capital_gl_bin', columns= 'y', aggfunc= 'count', values=['flag'],  fill_value=0
                  ).apply(lambda x: x/ x.sum() * 100, axis=1).plot.bar(stacked= True)

#Education

datadf.pivot_table(index= 'education_bin', columns= 'y', aggfunc= 'count', values=['flag'],  fill_value=0
                  ).apply(lambda x: x/ x.sum() * 100, axis=1).plot.bar(stacked= True)

#Hours per week

datadf.pivot_table(index= 'hours_per_week_bin', columns= 'y', aggfunc= 'count', values=['flag'],  fill_value=0
                  ).apply(lambda x: x/ x.sum() * 100, axis=1).plot.bar(stacked= True)



#Marriage Status and Relationship

datadf.pivot_table(index= 'msr_bin', columns= 'y', aggfunc= 'count', values=['flag'],  fill_value=0
                  ).apply(lambda x: x/ x.sum() * 100, axis=1).plot.bar(stacked= True)


#Occupation
datadf.pivot_table(index= 'occupation_bin', columns= 'y', aggfunc= 'count', values=['flag'],  fill_value=0
                  ).apply(lambda x: x/ x.sum() * 100, axis=1).plot.bar(stacked= True)


#Race and Sex

datadf.pivot_table(index= 'race_sex_bin', columns= 'y', aggfunc= 'count', values=['flag'],  fill_value=0
                  ).apply(lambda x: x/ x.sum() * 100, axis=1).plot.bar(stacked= True)


# **Decision Tree Classifier Models**

***Note to professor: I should've used x_train,y_train etc but I realized right in the middle.. so moving forward I will use that instead of 'target' and 'inputs'***

traindf.head()

#for training data
inputs = traindf.drop(['workclass_bin','flag','education_num_bin','y'],axis='columns')
target = traindf['y']



#for test data

test_inputs = testdf.drop(['workclass_bin','flag','education_num_bin','y'],axis='columns')
test_target = testdf['y']

#make data numerical 
from sklearn.preprocessing import LabelEncoder
le_hours_per_week_bin = LabelEncoder()
le_occupation_bin = LabelEncoder()
le_msr_bin = LabelEncoder()
le_capital_gl_bin = LabelEncoder()
le_race_sex_bin = LabelEncoder()
le_education_bin = LabelEncoder()
le_age_bin = LabelEncoder()


#for training data
inputs['hours_per_week_bin_n'] = le_hours_per_week_bin.fit_transform(inputs['hours_per_week_bin'])
inputs['occupation_bin_n'] = le_occupation_bin.fit_transform(inputs['occupation_bin'])
inputs['msr_bin_n'] = le_msr_bin.fit_transform(inputs['msr_bin'])
inputs['capital_gl_bin_n'] = le_capital_gl_bin.fit_transform(inputs['capital_gl_bin'])
inputs['race_sex_bin_n'] = le_race_sex_bin.fit_transform(inputs['race_sex_bin'])
inputs['age_bin_n'] = le_education_bin.fit_transform(inputs['age_bin'])
inputs['education_bin_n'] = le_age_bin.fit_transform(inputs['education_bin'])


inputs.head()

#for training data
inputs_n = inputs.drop(['hours_per_week_bin',	'occupation_bin',	'msr_bin',	'capital_gl_bin'	,'race_sex_bin'	,	'education_bin'	,	'age_bin'],axis='columns')


inputs_n

#for test data
test_inputs['hours_per_week_bin_n'] = le_hours_per_week_bin.fit_transform(test_inputs['hours_per_week_bin'])
test_inputs['occupation_bin_n'] = le_occupation_bin.fit_transform(test_inputs['occupation_bin'])
test_inputs['msr_bin_n'] = le_msr_bin.fit_transform(test_inputs['msr_bin'])
test_inputs['capital_gl_bin_n'] = le_capital_gl_bin.fit_transform(test_inputs['capital_gl_bin'])
test_inputs['race_sex_bin_n'] = le_race_sex_bin.fit_transform(test_inputs['race_sex_bin'])
test_inputs['age_bin_n'] = le_education_bin.fit_transform(test_inputs['age_bin'])
test_inputs['education_bin_n'] = le_age_bin.fit_transform(test_inputs['education_bin'])

test_inputs.head()

#for test data
test_inputs_n = test_inputs.drop(['hours_per_week_bin',	'occupation_bin',	'msr_bin',	'capital_gl_bin'	,'race_sex_bin'	,	'education_bin'	,	'age_bin'],axis='columns')


import time



start = time.time() #checking how long it takes the model to be done
from sklearn.tree import DecisionTreeClassifier
dtree = DecisionTreeClassifier(max_depth =10, random_state=101, max_features = None, min_samples_leaf = 15)

dtree.fit(inputs_n,target)

print("Total time: ", time.time() - start, "seconds")


#Prediction
y_pred = dtree.predict(test_inputs_n)


#lets see our score before
dtree.score(inputs_n,target)

# **Visualize Your Decision Tree using GraphViz**

from sklearn.externals.six import StringIO  
from IPython.display import Image  
from sklearn.tree import export_graphviz
import pydotplus
dot_data = StringIO()
export_graphviz(dtree, out_file=dot_data,  
                filled=True, rounded=True,
                special_characters=True)
graph = pydotplus.graph_from_dot_data(dot_data.getvalue())  
Image(graph.create_png())

# **Evaluate Decision Tree Performance** 

## **Confusion Matrix**

#confusion matrix

print(metrics.confusion_matrix(test_target, y_pred))

# save the confusion matrix & slice into four pieces
matrix = metrics.confusion_matrix(test_target, y_pred)
TP = matrix[1, 1]
TN = matrix[0, 0]
FP = matrix[0, 1]
FN = matrix[1, 0]

## **Classification Accuracy & F1**

#Classification Accuracy
print((TP + TN) / float(TP + TN + FP + FN))
print(metrics.accuracy_score(test_target, y_pred))

from sklearn.metrics import f1_score

#F1 Score
f1_score(test_target, y_pred, average='weighted')

## **Recall, Specificity, Precision, False Positive Rate**

#Sensitivity/Recall - When the actual value is positive, how often is the prediction correct?


print(TP / float(TP + FN))
print(metrics.recall_score(test_target, y_pred))

#Specificity - When the actual value is negative, how often is the prediction correct?


print(TN / float(TN + FP))

#False Positive Rate - When the actual value is negative, how often is the prediction incorrect?

print(FP / float(TN + FP))



#Precision - When a positive value is predicted, how often is the prediction correct?


print(TP / float(TP + FP))
print(metrics.precision_score(test_target, y_pred))

## **ROC Calculations & Curve**

#Import more libraries
from sklearn.metrics import roc_curve
from sklearn.metrics import roc_auc_score
import matplotlib.pyplot as plt


#Defining a python function to plot the ROC curves.
def plot_roc_curve(fpr, tpr):
    plt.plot(fpr, tpr, color='orange', label='ROC')
    plt.plot([0, 1], [0, 1], color='darkblue', linestyle='--')
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title('Receiver Operating Characteristic (ROC) Curve')
    plt.legend()
    plt.show()

#Predict probabilities for the test data.
probs = dtree.predict_proba(test_inputs_n)
probs


#Keep Probabilities of the positive class only.
probs = probs[:, 1]

#AUC Score.
auc = roc_auc_score(test_target, probs)
print('AUC: %.2f' % auc)

#ROC Curve
fpr, tpr, thresholds = roc_curve(test_target, probs)

#Plot ROC Curve
plot_roc_curve(fpr, tpr)


#cross-validated AUC
from sklearn.model_selection import cross_val_score

cross_val_score(dtree, inputs_n, target, cv=10, scoring='roc_auc').mean()

# **Tune Decision Tree Performance**

###***Tree 1***

#TREE 1

tree1 = DecisionTreeClassifier(criterion = 'entropy', min_samples_split= 10, min_samples_leaf = 10,max_depth =5, random_state=100 )

tree1.fit(inputs_n,target)


#Tree 1 prediction
y_pred1 = tree1.predict(test_inputs_n)

#Accuracy Score
print(metrics.accuracy_score(test_target, y_pred1))

#Recall
print(metrics.recall_score(test_target, y_pred1))

#Precision
print(metrics.precision_score(test_target, y_pred1))

#F1 Score

#F1 Score
f1_score(test_target, y_pred1, average='weighted')

###***Tree 2***

#TREE 2

tree2 = DecisionTreeClassifier(criterion = 'entropy', min_samples_split= 15, min_samples_leaf = 15,max_depth =15, random_state=100 )

tree2.fit(inputs_n,target)


#Tree 1 prediction
y_pred2 = tree2.predict(test_inputs_n)

#Accuracy Score
print(metrics.accuracy_score(test_target, y_pred2))

#Recall
print(metrics.recall_score(test_target, y_pred2))

#Precision
print(metrics.precision_score(test_target, y_pred2))


#F1 Score
f1_score(test_target, y_pred2, average='weighted')

###***Tree 3***

#TREE 3

tree3 = DecisionTreeClassifier(criterion = 'entropy', min_samples_split= 5, min_samples_leaf = 5,max_depth =40, random_state=100 )

tree3.fit(inputs_n,target)


#Tree 1 prediction
y_pred3 = tree3.predict(test_inputs_n)

#Accuracy Score
print(metrics.accuracy_score(test_target, y_pred3))

#Recall
print(metrics.recall_score(test_target, y_pred3))

#Precision
print(metrics.precision_score(test_target, y_pred3))


#F1 Score
f1_score(test_target, y_pred3, average='weighted')

###***Tree 4***

#TREE 4

tree4 = DecisionTreeClassifier(criterion = 'entropy', min_samples_split= 5, min_samples_leaf = 50,max_depth =200, random_state=100 )

tree4.fit(inputs_n,target)


#Tree 1 prediction
y_pred4 = tree4.predict(test_inputs_n)

#Accuracy Score
print(metrics.accuracy_score(test_target, y_pred4))

#Recall
print(metrics.recall_score(test_target, y_pred4))

#Precision
print(metrics.precision_score(test_target, y_pred4))


#F1 Score
f1_score(test_target, y_pred4, average='weighted')



###***Tree 5 - Gini***

#TREE 5

tree5 = DecisionTreeClassifier(criterion = 'gini', min_samples_split= 5, min_samples_leaf = 10,max_depth =10, random_state=100 )

tree5.fit(inputs_n,target)


#Tree 5 prediction
y_pred5 = tree5.predict(test_inputs_n)

#Accuracy Score
print(metrics.accuracy_score(test_target, y_pred5))

#Recall
print(metrics.recall_score(test_target, y_pred5))

#Precision
print(metrics.precision_score(test_target, y_pred5))


#F1 Score
f1_score(test_target, y_pred5, average='weighted')



###***Tree 6 - Gini***

#TREE 6

tree6 = DecisionTreeClassifier(criterion = 'gini', min_samples_split= 15, min_samples_leaf = 50,max_depth =100, random_state=100)

tree6.fit(inputs_n,target)


#Tree 1 prediction
y_pred6 = tree6.predict(test_inputs_n)

#Accuracy Score
print(metrics.accuracy_score(test_target, y_pred6))

#Recall
print(metrics.recall_score(test_target, y_pred6))

#Precision
print(metrics.precision_score(test_target, y_pred6))


#F1 Score
f1_score(test_target, y_pred6, average='weighted')



###***Tree 7 - Gini***

#TREE 7

tree7 = DecisionTreeClassifier(criterion = 'gini', min_samples_split= 5, min_samples_leaf = 15,max_depth =300, random_state=100 )

tree7.fit(inputs_n,target)


#Tree 1 prediction
y_pred7 = tree7.predict(test_inputs_n)

#Accuracy Score
print(metrics.accuracy_score(test_target, y_pred7))

#Recall
print(metrics.recall_score(test_target, y_pred7))

#Precision
print(metrics.precision_score(test_target, y_pred7))


#F1 Score
f1_score(test_target, y_pred7, average='weighted')



###***Tree 8 - Gini***

#TREE 8

tree8 = DecisionTreeClassifier(criterion = 'gini', min_samples_split= 5, min_samples_leaf = 100,max_depth =50, random_state=100 )

tree8.fit(inputs_n,target)


#Tree 1 prediction
y_pred8 = tree8.predict(test_inputs_n)

#Accuracy Score
print(metrics.accuracy_score(test_target, y_pred8))

#Recall
print(metrics.recall_score(test_target, y_pred8))

#Precision
print(metrics.precision_score(test_target, y_pred8))


#F1 Score
f1_score(test_target, y_pred8, average='weighted')

# **Conclusion**

**Q.8.1 How long was your total run time to train the model?**

> Total time:  0.03493022918701172 seconds to train the model



**Q.8.2 Did you find the BEST TREE?**



> Yes, Tree 5 using Gini Impurity, minimum sample split 5, minimum sample leaf 10 and Maximum Depth 10



**Q.8.3 Draw the Graph of the BEST TREE Using GraphViz**

dot_data = StringIO()
export_graphviz(tree5, out_file=dot_data,  
                filled=True, rounded=True,
                special_characters=True)
graph = pydotplus.graph_from_dot_data(dot_data.getvalue())  
Image(graph.create_png())

**Q.8.4 What makes it the best tree?**

it has slightly better values than the rest although not much different than Decision Tree #4. It's got a higher acccuracy and precision score

# **Automation of Performance Tuning**

hyperparameters = pd.read_csv('/content/drive/MyDrive/MSBA_Assignments/CA03/treetuning.csv')
hyperparameters = hyperparameters.rename(columns={"Split Criteria (Entropy or Gini)":"criterion", "Minimum Sample Split":"min_samples_split","Minimum Sample Leaf":"min_samples_leaf",
                                "Maximum Depth":"max_depth"})

#making it easier to read for myself
hyperparameters['criterion'] = np.where((hyperparameters.criterion == 'Entropy'),'entropy',hyperparameters.criterion)
hyperparameters['criterion'] = np.where((hyperparameters.criterion == 'Gini Impurity'),'gini',hyperparameters.criterion)

def autotune():
  #to identify the different results
  treenum=1
  #loop to iterrate through each row
  for index, row in hyperparameters.iterrows():
    treeauto = DecisionTreeClassifier(criterion = row['criterion'], min_samples_split= row['min_samples_split'], min_samples_leaf = row['min_samples_leaf'],max_depth =row['max_depth'], random_state=100 )
    treeauto.fit(inputs_n,target)
    y_pred_auto = treeauto.predict(test_inputs_n)
    print(f"Results of Tree {treenum} are:")
    print(f"Accuracy score is {metrics.accuracy_score(test_target, y_pred_auto)}.")
    print(f"Recall is {metrics.recall_score(test_target, y_pred_auto)}")
    print(f"Precision is {metrics.precision_score(test_target, y_pred_auto)}")
    print(f"F1 Score is {f1_score(test_target, y_pred_auto, average='weighted')}")
    print("\n")
    treenum=treenum+1

autotune()

# **Prediction using my “trained” Decision Tree Model**

#creating empty dataset to append values for new person
new_inputs = test_inputs_n.iloc[0:0]

#adding new person's values
df = {'hours_per_week_bin_n':3, 
        'occupation_bin_n':1, 
        'msr_bin_n':2,
        'capital_gl_bin_n':2,
        'race_sex_bin_n':1,
        'age_bin_n':3,
        'education_bin_n':4,
       } 

new_inputs = new_inputs.append(df, ignore_index = True) 

#with Tree 5 prediction
y_pred_new = tree5.predict(new_inputs)
y_pred_new



***Prediction is that the individual will earn >50K***

print(f"Accuracy score is {metrics.accuracy_score(test_target, y_pred_auto)}.")
print(f"Recall is {metrics.recall_score(test_target, y_pred_auto)}")
print(f"Precision is {metrics.precision_score(test_target, y_pred_auto)}")
print(f"F1 Score is {f1_score(test_target, y_pred_auto, average='weighted')}")
